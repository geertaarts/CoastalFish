# INLA MODEL FITTING

## Making the model formula and running the INLA model

For the moment a model is fitted to data from the last 5 years up to 2017 (i.e. 2013-2017). These data are subsequently divided into a model data set and a validation data set. 


if (length(the.years)>1)  write.csv(dat.mod,file.path(PMR.data.path,paste("DFS/dfs4pmr_trllst_ENV_",Sys.Date(),".csv",sep="")))   
if (length(the.years)>1)  write.csv(dat.pred,file.path(PMR.data.path,paste("DFS/dfs4pmr_xy_",Sys.Date(),".csv",sep="")))

```{r, eval=TRUE, echo=T}

Loc <- cbind(dat$x_utm_km, dat$y_utm_km) #km
set.seed(2)

fit_ind <- which(dat$sampleID!="2017_predict")

A1 <- inla.spde.make.A(mesh = mesh1c, loc=Loc[fit_ind,])

A2 <- inla.spde.make.A(mesh = mesh1c, loc=Loc[-fit_ind,])

N <- length(fit_ind)
Stack1 <- inla.stack(
  tag  = "Fit",
  data = list(y = dat$count[fit_ind],offset.arg=dat$the.offset[fit_ind]),    
  A    = list(1,1, A1),         
  effects = list( 
    Intercept=rep(1,N),
    X=X[fit_ind,], #Covariates
    w=w.st))                  #Spatial field

N <- nrow(dat)-length(fit_ind)
Stack2 <- inla.stack(
  tag  = "pred",
  data = list(y = NA,offset.arg=dat$the.offset[-fit_ind]),    
  A    = list(1,1, A2),         
  effects = list( 
    Intercept=rep(1,N),
    X=X[-fit_ind,], #Covariates
    w=w.st))                  #Spatial field

stk.full <- inla.stack(Stack1,Stack2)

```


```{r, eval=FALSE, echo=T}
A <- inla.spde.make.A(mesh = mesh1c, loc=Loc)

N <- nrow(dat)
stk.full <- inla.stack(
  tag  = "Fit",
  data = list(y = dat$count,offset.arg=dat$the.offset),    
  A    = list(1,1, A),         
  effects = list( 
    Intercept=rep(1,N),
    X=X, #Covariates
    w=w.st))                  #Spatial field
```

p.res <- inla(formula, data=inla.stack.data(stk.full), ## full stack
              control.predictor=list(compute=TRUE, ## compute the predictor
                                     A=inla.stack.A(stk.full))) ## using full st

The model formula used in the inla model is generated from the names of the model matrix, combined with the intercept term and the spatial correlation model ("f(w, model=spde)").

Subsequently, two inla models are run, one assuming that the data are Poisson distributed, and another model assuming that the data are negative binomial distributed.


# What to do is calculate how the deviance information criteria is estimated. 
How are the Devicance Information Criteria (DIC) and The Watanabe-Akaike information criterion (WAIC) computed?
  The DIC criteria depends on the deviance, -2*log.likelihood, and we need to compute the posterior expectation of it, and evaluate it at the posterior expectation.  The calculations are straight forward, but instead of evaluating the deviance at the posterior mean of all parameters, we evaluate the deviance at the posterior mean of the latent field (x), and the posterior mode of the hyperparameters (theta). The reason is that the posterior marginals for the hyperparameters, like the precision, say, can be highly skewed, so that the posterior expectation is not at all a good representation of location. This 'almost' corresponds to a 'good' reparameterisation of the hyperparameters, like from precision to log(precision), as the posterior marginal for log(precision) is much much more symmetric.

The WAIC criterion is defined and discussed in this tech-report and we follow their recomandation to use p.eff as in their equation (11).  

In order to compute this criteria, use 

r=inla(..., control.compute = list(dic = TRUE, waic = TRUE))

and then the results is available as r$dic and r$waic

See tech report below on cross validation/model selection

https://78462f86-a-ea572c08-s-sites.googlegroups.com/a/r-inla.org/www/faq/waic_understand3.pdf?attachauth=ANoY7cr0AIYpqdu5EQkIH2KkEpzyrQyDWwkC2q2XiLY7FKHlF3Zds4UMGZyLiAFr8KRZQAzkdnuDKc_5krPJAsEDHfb-KQKXa4LM_Bt7kRdeVIYrYlsH_CJyX113l6O-tHuDORgInOrO41h7IVcmgq3ke7zPLj2yPFkQeQWbIGZVhfCEbTsQFPfZEJ_Pna8XHKOI4LH8G7Gi7Xt1a6skT1bYWJKqCj2zMw%3D%3D&attredirects=0


```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}

var.names=c("day_of_year","Gar_8_9","Pkor_8_9","Bkor_8_9","slib","zand",
"dz10","dz60","tpi_3","tpi_9","tpi_15","wind_force","factor_year")
  
var.names<-Expl.Var$X
var.remaining<-var.names
var.in.model<-c()

store.table<-data.frame(var=var.names,cpo.1=NA,cpo.2=NA,cpo.3=NA, cpo.4=NA,cpo.5=NA,cpo.6=NA,cpo.7=NA)
store.table$var<-as.character(store.table$var)

store.table.dic<-store.table

for (j in 1:7){
  print(j)
for (var.name in var.remaining){
  print(var.name)

model.vars<-c(var.in.model,var.name)
if (is.element(var.name,c("zand","gravel","slib","dz10","dz60"))) model.vars<-c(model.vars,"missing")
model.vars<-model.vars[duplicated(model.vars)==FALSE]

col.names<-names(X)[grep(paste(c(model.vars),collapse="|"),names(X))]
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(col.names," f(w, model = spde)","offset(the.offset)"),collapse =" + ")))
INLA:::inla.dynload.workaround() 

I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
             control.compute = list(dic = TRUE, waic = TRUE, config=TRUE, cpo=TRUE),#offset=offset.arg,
             control.predictor = list(A = inla.stack.A(Stack1)))

row.nr<-which(var.name==store.table$var)
sum.cpo<-sum(log(I1nb$cpo$cpo),na.rm=TRUE)
print(sum.cpo)
print(I1nb$dic$dic)

store.table[row.nr,paste("cpo",j,sep=".")]<-sum.cpo
store.table.dic[row.nr,paste("cpo",j,sep=".")]<-I1nb$dic$dic

#print(store.table[i,])
}
#best.var<-store.table$var[which.max(store.table[,paste("cpo",j,sep=".")])]
best.var<-store.table.dic$var[which.min(store.table.dic[,paste("cpo",j,sep=".")])]
var.remaining<-var.remaining[-which(var.remaining==best.var)]
var.in.model<-c(var.in.model,best.var)

}
```

```{r, eval=F, results='hide', echo=T, message=FALSE, warning=FALSE}
#plot(0,0,xlim=range(c(store.table[]))
#print(store.table[i,])


```


```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}
model.vars=c("wind_force","year","day_of_year","Bkor_8_9","tauw_xxxx_veld.nc_7d")
model.vars=var.in.model
var.names<-col.names<-names(X)[grep(paste(c(model.vars),collapse="|"),names(X))]
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(col.names," f(w, model = spde)","offset(the.offset)"),collapse =" + ")))

I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
             control.compute = list(dic = TRUE, waic = TRUE, config=TRUE, cpo=TRUE),#offset=offset.arg,
             control.predictor = list(A = inla.stack.A(Stack1)))

model.vars=c("sal_layer10_xxxx.nc_7d")
var.names<-col.names<-names(X)[grep(paste(c(model.vars),collapse="|"),names(X))]

pred<-as.matrix(X[,var.names])%*%array(I1nb$summary.fixed[var.names,"mean"])
#plot(X[,var.names[1]],pred)
plot(dat[,model.vars],pred)

set.seed(1234)
NSim <- 1000
Sim <- inla.posterior.sample(n = NSim, result = I1nb)[1]


wrownum <- grep(paste(c(model.vars),collapse="|"),rownames(Sim[[1]]$latent))
wmat <- sapply(Sim, function(x) {x$latent[wrownum]})
dim(wmat)


pred2<-as.matrix(X[,var.names])%*%wmat
lower<-c(apply(pred2,1,function(x){quantile(x,0.05)}))
upper<-c(apply(pred2,1,function(x){quantile(x,0.95)}))
pred.dat<-data.frame(x=dat[,model.vars],mean=pred,lower=lower,upper=upper)
pred.dat<-pred.dat[order(pred.dat$x),]
plot(pred.dat$x,pred.dat$mean,type="line")
rug(pred.dat$x)
lines(pred.dat$x,pred.dat$lower,lty=3)
lines(pred.dat$x,pred.dat$upper,lty=3)

```
 
Do spatial prediction


```{r, eval=FALSE, echo=T}


model.vars=c("wind_force","year","day_of_year","Bkor_8_9","tauw_xxxx_veld.nc_7d")
model.vars=var.in.model
var.names<-col.names<-names(X)[grep(paste(c(model.vars),collapse="|"),names(X))]
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(col.names," f(w, model = spde)","offset(the.offset)"),collapse =" + ")))

I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(stk.full),
             control.compute = list(dic = TRUE, waic = TRUE, config=TRUE, cpo=TRUE),#offset=offset.arg,
             control.predictor = list(A = inla.stack.A(stk.full)))

pred<-I1nb$summary.fitted.values[1:nrow(dat),][-fit_ind,]
kleur<-pred$mean-(min(pred$mean))
kleur<-round(100*kleur/diff(range(kleur)))+1
plot(dat$lon[-fit_ind],dat$lat[-fit_ind],col=rev(rainbow(120))[kleur],xlab="Longitude", ylab="Lattitude", main=paste(the.species, "  N/haul Age ",the.age,sep="",lwd=3))
points(trawl.s$lon,trawl.s$lat, 
     cex=(0.2+log(trawl.s$NatA+1)/3), 
     col=c('black'),
     xlab="Longitude", ylab="Lattitude", main=paste(the.species, "  N/haul Age ",the.age,sep=""))
plot(land,add=TRUE,border="grey75",col="grey90")


```


```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}
dat.pred<-read.csv(file.path(PMR.data.path,"DFS/dfs4pmr_xy_2019-11-01.csv"))
dat<-r

# Extract prediction values: mean and confidence intervals

# Convert to matrix and raster

# Plot mean and SD

# If possible add raw count data
```



Sim[[1]]$latent[var.names,]

rnames <- rownames(Sim[[1]]$latent)
rtypes <- unique(unlist(lapply(strsplit(rnames,":"),function(x){x[1]})))
rtypes

rtypes[1]
wrownum <- grep(paste0("^",rtypes[1]),rownames(Sim[[1]]$latent))
wmat <- sapply(Sim, function(x) {x$latent[wrownum]})
dim(wmat)



```

which(I1nb$summary.fitted.values$mean[1:nrow(dat.s)]>1e+10)
plot(dat.s$counts/exp(dat.s$the.offset),I1nb$summary.fitted.values$mean[1:nrow(dat.s)],ylim=c(0,100))


sum(I1nb$dic$local.dic)
sum(I1nb$dic$local.dic[fit_ind])
sum(I1nb$dic$local.dic[-fit_ind])
I1nb$dic$dic

pred.ind <- inla.stack.index(stk.full, tag = "pred")$data
fit.ind <- inla.stack.index(stk.full, tag = "Fit")$data

# Is below with or without offset, check by multiplying offset by 100 and see how this effects the results
ypost <- I1nb$marginals.fitted.values[c(fit.ind,pred.ind)]


x<-c()
for(i in 1:length(ypost)){
  
  x<-c(x,ypost[[i]][,"x"][which(ypost[[i]][,"y"]==max(ypost[[i]][,"y"]))[1]])
  
}
plot(dat.s$count[-fit_ind],x)
plot(dat.s$count[-fit_ind],exp(x))

summary(I1nb$summary.fitted.values$mean[1:nrow(dat.s)])

# below does not deal with offset
summary(gam(dat.s$count[-fit_ind]~x,family="nb"))

plot(dat.s$count[-fit_ind],exp(x))


plot(ypost[[1]][,"x"],ypost[[1]][,"y"])
abline(v=dat.s$count[-fit_ind][1])


fsp.2 <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)[!names(X)=="the.offset"][1:5]," f(w, model = spde)"),collapse =" + ")))

I1nb.2 <- inla(eval(fsp.2), family = "nbinomial", data=inla.stack.data(stk.full),
               E=inla.stack.data(stk.full)$e,
               control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
               control.predictor = list(A = inla.stack.A(stk.full)))


I1p <- inla(eval(fsp), family = "poisson", data=inla.stack.data(Stack1),
            control.compute = list(dic = TRUE, waic = TRUE),
            control.predictor = list(A = inla.stack.A(Stack1)))
I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
             control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
             control.predictor = list(A = inla.stack.A(Stack1)))
I1zip <- inla(eval(fsp), family = "zeroinflatedpoisson1", data=inla.stack.data(Stack1),
              control.compute = list(dic = TRUE, waic = TRUE),
              control.predictor = list(A = inla.stack.A(Stack1)))
I1zinb <- inla(eval(fsp), family = "zeroinflatednbinomial1", data=inla.stack.data(Stack1),
               control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
               control.predictor = list(A = inla.stack.A(Stack1)))
```
Once the INLA models are run, a summary can be printed for the models. This summary contains much of the relevant information for the models. The summary also contains the DIC and WAIC information criterions. DIC and WAIC work just like AIC in that the model with the lowest WAIC should be selected.

```{r, eval=T, echo=T}
summary(I1p)$waic$waic
```

The Poisson model thus has a WAIC value of `r summary(I1p)$waic$waic`. We can make a simple table to compare the WAIC values of the different models.

```{r, eval=T, echo=T}
dic  <- c(I1p$dic$dic, I1nb$dic$dic, I1zip$dic$dic, I1zinb$dic$dic)
waic <- c(I1p$waic$waic, I1nb$waic$waic, I1zip$waic$waic, I1zinb$waic$waic)
Z     <- cbind(dic, waic)
rownames(Z) <- c("Poisson model", "Negative binomial model", "Zero Inflated Poisson model", "Zero Inflated Negative binomial model" )
Z
```

The negative binomial model has the lowest DIC and WAIC. Let's see the summary of this model.  
```{r, eval=T, echo=T}
summary(I1nb)
```
The summary displays the call to INLA, some information about the time needed to process the model, information about the fixed effects, random effects, model hyperparameters, and several information criteria. The fixed effects include the intercept and all parameter valuse for the factor variable years. The number of parameter estimates for years is equal the the total number of years in the model minus 1. Likewise, there are two surveys in the data, and the number of parameter estimates in the model is two minus one: only the parameter estimate for the difference between the IBTS and BTS is given. The parameter estimate named "lhauldur" gives the linear predictor for the effect of log haul duration.

The model output indicates that there are random effects for the SPDE2 model.

In this case the model "hyperparameters" provide information about the overdispersion parameter in the negative binomial distribution. Remember that the variance in a negative binomial distribution is equal to $\mu + \mu^{2} / k$. The "size" hyperparameter provides the estimate of $k$. In our case, the estimate of "size" is `r I1nb$summary.hyperpar$mean[1]` and the data is overdispersed (1/overdispersion < 1). 

The two theta parameters describe the spatial field w. Theta2 is equal to  $\log(\kappa)$. Theta1 is equal to $\log(\tau)$ = - $\log(4 \pi \kappa^2 \sigma_{u}^2)/2$. Kappa is a parameter in the Matern correlation function, that defines the rate of decline in the correlation. Large $\kappa$ values indicate a fast decline in correlation. The Matern correlation function contains two parameters: $\kappa$ and $\nu$. While $\kappa$ is being estimates, $\nu$ is assumed to be equal to 1 by default in INLA. 

Plot the histograms of observations and fits for all models. 

```{r, eval=T, echo=T, dpi=600}
idx <- inla.stack.index(Stack1, tag= 'Fit')$data
par(mfrow=c(2,2))
yl <- c(0,log(max(table(dat.s$count),na.rm=T))*1.3)
names.arg <- as.numeric(names(table(dat.s$count)))
breaks <- c(as.numeric(names(table(dat.s$count))),1000)
ltable <- log(table(dat.s$count))
pos <- barplot(ltable, names.arg = names.arg, ylim = yl, main = "Poisson model")
points(y=log(table(cut(I1p$summary.fitted.values[idx,"mean"], breaks=breaks))), x = pos[,1], col=2, pch=19)
pos <- barplot(ltable, names.arg = names.arg, ylim = yl, main = "Negative binomial model")
points(y=log(table(cut(I1nb$summary.fitted.values[idx,"mean"], breaks=breaks))), x = pos[,1], col=2, pch=19)
pos <- barplot(ltable, names.arg = names.arg, ylim = yl, main = "ZIP model")
points(y=log(table(cut(I1zip$summary.fitted.values[idx,"mean"], breaks=breaks))), x = pos[,1], col=2, pch=19)
pos <- barplot(ltable, names.arg = names.arg, ylim = yl, main = "ZINB model")
points(y=log(table(cut(I1zinb$summary.fitted.values[idx,"mean"], breaks=breaks))), x = pos[,1], col=2, pch=19)
```

We mentioned the two theta hyperparamaters earlier, and how they contain the information about the spatial correlation in the model. These hyperparameters and some derived hyperparameters from the spde can be extracted from the model using inla.spde2.result()

```{r, eval=T, echo=T}
# extract spde2 results 
SpatField.w <- inla.spde2.result(inla = I1nb, name = "w", spde = spde, do.transfer = TRUE)
```

This object contains the hyperparameters and the information about e.g. $\kappa$ and $\sigma_{u}$.
The parameter $\kappa$ defines the rate of decline in correlation, and the parameters $\sigma_{u}^2$ indicates the variance. Below, we use the inla.emarginal function of inla to compute the expected values for the hyperparameters.   

```{r, eval=T, echo=T}
# Spatial info
Kappa <- inla.emarginal(function(x) x, 
SpatField.w$marginals.kappa[[1]] )
Sigma_u <- inla.emarginal(function(x) sqrt(x), 
SpatField.w$marginals.variance.nominal[[1]] )
range <- inla.emarginal(function(x) x, 
SpatField.w$marginals.range.nominal[[1]] )
c(Kappa, Sigma_u, range)
```

The posterior mean of $\kappa$ is `r Kappa` and the posterior mean $\sigma_{u}$ is `r Sigma_u`. The range is the distance at which the spatial correlation is approxmately 0.1. In this case, this range is approximately `r round(range)' km. Below we plot the correlation as a function of distance, with a dashed vertical line at the range and a dashed horizontal line where the correlation is exactly 0.1.
```{r, eval=T, echo=T,  dpi=600}
# Show correlation structure
LocMesh <- mesh1a$loc[,1:2]
# And then we calculate the distance between each vertex.
D <- as.matrix(dist(LocMesh))
# Using the estimated parameters from the model (see above)
# we can calculate the imposed Matern correlation values.
d.vec <- seq(0, max(D), length = 100)      
Cor.M <- (Kappa * d.vec) * besselK(Kappa * d.vec, 1) 
#Cor.M[1] <- 1
# Which we plot here:
par(mfrow=c(1,1))
plot(x = d.vec, y = Cor.M, 
     type = "l", 
     xlab = "Distance (km)", 
     ylab = "Correlation")
abline(h = 0.1, lty = 2)
abline(v = range, lty = 2)
```


```{r, eval=T, echo=F, fig.width=8, fig.height=4, dpi=600}
wproj <- inla.mesh.projector(mesh1c, xlim = range(Loc[,1]), ylim = range(Loc[,2])) 
wm.pm100100  <- inla.mesh.project(wproj, I1nb$summary.random$w$mean)
wsd.pm100100 <- inla.mesh.project(wproj, I1nb$summary.random$w$sd)

grid     <- expand.grid(x = wproj$x, y = wproj$y)
grid$zm  <- as.vector(wm.pm100100)   
grid$zsd <- as.vector(wsd.pm100100)   
p1 <- levelplot(zm ~ x * y,
                data = grid, 
                scales = list(draw = TRUE),
                xlab = list("Easting", cex = 1),
                ylab = list("Northing", cex = 1),
                main = list("Posterior mean spatial random fields", cex = 1),
                col.regions=tim.colors(25, alpha = 1),
                panel=function(x, y, z, subscripts,...){
                  panel.levelplot(x, y, z, subscripts,...)
                  grid.points(x = dat.s$x_utm_km,
                              y = dat.s$y_utm_km, 
                              pch = 1,
                              size = unit(dat.s$count/10, "char"))  
                }) + xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')
p2 <- levelplot(zsd ~ x * y,
                data = grid, 
                scales = list(draw = TRUE),
                xlab = list("Easting", cex = 1),
                ylab = list("Northing", cex = 1),
                main = list("Posterior sd spatial random fields", cex = 1),
                col.regions=tim.colors(25, alpha = 1),
                panel=function(x, y, z, subscripts,...){
                  panel.levelplot(x, y, z, subscripts,...)
                  
                }) + xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')
grid.arrange(p1,p2, ncol=2)

```

Next steps to take:
  
  - Test for inclusion of offset, use simulations with latent field, and use negative binomial and Poisson. See also http://www.r-inla.org/models/tools

- Include smooth functions of covariates. Options are polynomials (poly function) or function SmoothCon (mgcv package?)

- First use fewer covariates in the model, e.g. only use s(depth), s(dz10) and missing

- Create a regular prediction grid. One way to do this is to create a convex hull around the data-points and only use within the convex hull. Remove points with missing covariates

- Write model validation code. Similar to prediction table, the way to do this is to add a stack 'predict' and use the actual observations. Next use the correct likelihood function to test 

- Include data exploration, like histogram of response data and dotplots of explanatory variables

- Include ICES areas, see mail Loes

- Include fishing effort maps

Open questions:
  
  
  - Should we include sediment data from inner-Delta and Wadden Sea? Perhaps use subtidal SIBES? Not for now. In future analysis, use perhaps use SIBES data

- what are the other covariates to include? I.e. temperature, salinity, current velocity? Yes, but wait for ARCADIS data if the come available.

- Include seal predation pressure? Not for this analysis

- Use tidal data: Tidal data may influence both abundance (e.g. movement to shallow areas during high tide), but also catchability. E.g. vertical movement depending on tide. Chen will include this, so wait for her analysis, and based on outcome, possibly include.

- For sandeel, heads are measured, and head-length measurements could be used to reconstruct length. 